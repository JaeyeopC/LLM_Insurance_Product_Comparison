{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [summarization evaluation - reference link](https://www.trulens.org/cookbook/use_cases/summarization_eval/#dependencies) : Ground Truth Evaluation \n",
    "In this example, instead of query, content(dialog) is used as the input for the prompt. \n",
    "The summary is evaluated by\n",
    "- Ground Truth \n",
    "- Groundedness ( Tokeninzed )\n",
    "- Comprehensiveness ( Tokeninzed )\n",
    "- BERT Score \n",
    "- BLEU \n",
    "- ROUGE\n",
    "\n",
    "However, BLEU is not relevant to our application.   \n",
    "For the evaluation metrics, refer to the ```3. Evaluation_Trulens_GroundTruth_Categorization.ipynb```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Why is this exmample related to our task?  \n",
    "1. groundedness for summarization? and why it is relevant to our application?   \n",
    "Later on, we will compare different different product and I assumed that the output will be in a form of summarization of two products.   \n",
    "The prompts will take several variables from different products, and then the output will be summarization of the prompts.   \n",
    "\n",
    "For example, \n",
    "- Prompt : “What is the strength of company A over company B? Here are the information of two companies : {information of two companies will be inserted as a variable here} “ \n",
    "- Response : “Company A has a, b, c while Company B has b, e, f. Thus Company A has strength in a, c cases, and company B has strength in e ,f. Both companies offer b.” \n",
    "\n",
    "So this is more or less similar to summarization task.\n",
    "\n",
    "2. ground truth   \n",
    "We will also have a generated response for the prompt above.  \n",
    "It is reasonable to compare the previously generated response with the generated response.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics \n",
    "- Metrics that can be used for summarization ( for other features of insurance product ) \n",
    "    - ROUGE : Recall-Oriented Understudy for Gisting Evaluation is a group of metrics that evaluate LLM summarization and NLP (natural language processing) translations. It also uses a numerical scale from 0 to 1.\n",
    "    - [Bert Score](https://huggingface.co/spaces/evaluate-metric/bertscore)\n",
    "    - Groundedness ( Hallucination check )\n",
    "        - llm : measures whether the model's response is grounded in the input query/context. \n",
    "        - nli : measure the groundedness of the model's response using natural language inference.\n",
    "        - [trulens.providers.huggingface - groundedness_measure_with_nli](https://www.trulens.org/reference/trulens/providers/huggingface/?h=groundedness#trulens.providers.huggingface.Huggingface.groundedness_measure_with_nli)   / [groundedness_measure_with_cot_reasons](https://www.trulens.org/reference/trulens/feedback/?h=groundedness_measure_with_#trulens.feedback.LLMProvider.groundedness_measure_with_cot_reasons) \n",
    "        - [groundedness evaluation - reference link](https://www.trulens.org/component_guides/evaluation_benchmarks/groundedness_benchmark/?h=groundedness#benchmarking-various-groundedness-feedback-function-providers-openai-gpt-35-turbo-vs-gpt-4-vs-huggingface) : just for reference\n",
    "        - [summarization evaluation - reference link](https://www.trulens.org/cookbook/use_cases/summarization_eval/#dependencies) : can be adopted for summarization evaluation \n",
    "    - Ground Truth Agreement : compare the similiarity between the model's response and the ground truth. \n",
    "        - accuracy : 0 ~ 1 depending on the exactness of the response \n",
    "        - In general, ground truth evaluation only takes the input and the response into account. Thus, for a non-RAG application like this, it best suits the purpose. \n",
    "- Metrics not relevant to our application \n",
    "    - Perplexity \n",
    "    - BLEU : Bilingual Evaluation Understudy evaluates the precision of LLM-generated text, or how closely it resembles human sources, using a numerical scale from 0 to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading example dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  460k  100  460k    0     0  1384k      0 --:--:-- --:--:-- --:--:-- 1387k\n"
     ]
    }
   ],
   "source": [
    "!curl -o dialogsum.dev.jsonl https://raw.githubusercontent.com/cylnlp/dialogsum/main/DialogSum_Data/dialogsum.dev.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path_dev = \"dialogsum.dev.jsonl\"\n",
    "dev_df = pd.read_json(path_or_buf=file_path_dev, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dev_0</td>\n",
       "      <td>#Person1#: Hello, how are you doing today?\\n#P...</td>\n",
       "      <td>#Person2# has trouble breathing. The doctor as...</td>\n",
       "      <td>see a doctor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev_1</td>\n",
       "      <td>#Person1#: Hey Jimmy. Let's go workout later t...</td>\n",
       "      <td>#Person1# invites Jimmy to go workout and pers...</td>\n",
       "      <td>do exercise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev_2</td>\n",
       "      <td>#Person1#: I need to stop eating such unhealth...</td>\n",
       "      <td>#Person1# plans to stop eating unhealthy foods...</td>\n",
       "      <td>healthy foods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dev_3</td>\n",
       "      <td>#Person1#: Do you believe in UFOs?\\n#Person2#:...</td>\n",
       "      <td>#Person2# believes in UFOs and can see them in...</td>\n",
       "      <td>UFOs and aliens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dev_4</td>\n",
       "      <td>#Person1#: Did you go to school today?\\n#Perso...</td>\n",
       "      <td>#Person1# didn't go to school today. #Person2#...</td>\n",
       "      <td>go to school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dev_5</td>\n",
       "      <td>#Person1#: Honey, I think you should quit smok...</td>\n",
       "      <td>#Person1# asks #Person2# to quit smoking for h...</td>\n",
       "      <td>quit smoking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dev_6</td>\n",
       "      <td>#Person1#: Excuse me, Mr. White? I just need y...</td>\n",
       "      <td>Sherry reminds Mr. White to sign.</td>\n",
       "      <td>workplace conversation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dev_7</td>\n",
       "      <td>#Person1#: Hey, Karen. Look like you got some ...</td>\n",
       "      <td>#Person1# asks Karen where Karen stayed and ho...</td>\n",
       "      <td>holidays</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dev_8</td>\n",
       "      <td>#Person1#: How do you usually spend your leisu...</td>\n",
       "      <td>#Person1# asks about #Person2#'s hobbies. #Per...</td>\n",
       "      <td>hobby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dev_9</td>\n",
       "      <td>#Person1#: have you ever seen Bill Gate's home...</td>\n",
       "      <td>#Person1# and #Person2# talk about Bill Gate's...</td>\n",
       "      <td>dream home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fname                                           dialogue  \\\n",
       "0  dev_0  #Person1#: Hello, how are you doing today?\\n#P...   \n",
       "1  dev_1  #Person1#: Hey Jimmy. Let's go workout later t...   \n",
       "2  dev_2  #Person1#: I need to stop eating such unhealth...   \n",
       "3  dev_3  #Person1#: Do you believe in UFOs?\\n#Person2#:...   \n",
       "4  dev_4  #Person1#: Did you go to school today?\\n#Perso...   \n",
       "5  dev_5  #Person1#: Honey, I think you should quit smok...   \n",
       "6  dev_6  #Person1#: Excuse me, Mr. White? I just need y...   \n",
       "7  dev_7  #Person1#: Hey, Karen. Look like you got some ...   \n",
       "8  dev_8  #Person1#: How do you usually spend your leisu...   \n",
       "9  dev_9  #Person1#: have you ever seen Bill Gate's home...   \n",
       "\n",
       "                                             summary                   topic  \n",
       "0  #Person2# has trouble breathing. The doctor as...            see a doctor  \n",
       "1  #Person1# invites Jimmy to go workout and pers...             do exercise  \n",
       "2  #Person1# plans to stop eating unhealthy foods...           healthy foods  \n",
       "3  #Person2# believes in UFOs and can see them in...         UFOs and aliens  \n",
       "4  #Person1# didn't go to school today. #Person2#...            go to school  \n",
       "5  #Person1# asks #Person2# to quit smoking for h...            quit smoking  \n",
       "6                  Sherry reminds Mr. White to sign.  workplace conversation  \n",
       "7  #Person1# asks Karen where Karen stayed and ho...                holidays  \n",
       "8  #Person1# asks about #Person2#'s hobbies. #Per...                   hobby  \n",
       "9  #Person1# and #Person2# talk about Bill Gate's...              dream home  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple summarization app and instrument it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables and initialize clients\n",
    "load_dotenv()\n",
    "OpenAI_key = os.getenv(\"OPENAI_API_KEY\")  \n",
    "Huggingface_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "from trulens.apps.custom import instrument\n",
    "import openai\n",
    "\n",
    "\n",
    "# [TODO] : change the code to fit our application \n",
    "class DialogSummaryApp:\n",
    "    @instrument\n",
    "    def summarize(self, dialog): \n",
    "        client = openai.OpenAI() # [TODO] ChatPromptTemplate으로 변경 / company_name\": row['company'],\"title\": row['title'], \"content\": row['content'], \"format_instructions\": format_instructions 매개변수로 받아 넘김 / for문으로 앱 호출 / response(cateogry) 반환 \n",
    "        summary = (\n",
    "            client.chat.completions.create( #ChatPromptTemplate 으로 변경 가능? \n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"Summarize the given dialog into 1-2 sentences based on the following criteria:  \n",
    "                        1. Convey only the most salient information; \n",
    "                        2. Be brief; \n",
    "                        3. Preserve important named entities within the conversation; \n",
    "                        4. Be written from an observer perspective; \n",
    "                        5. Be written in formal language. \"\"\", # [TODO] : replace the content with \"system prompt text\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": dialog},  # [TODO] : replace content with \"user prompt text\" ( categorization query )\n",
    "                ],\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Database and View Dashboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
      "Updating app_id in records table: 0it [00:00, ?it/s]\n",
      "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from trulens.core import TruSession\n",
    "from trulens.dashboard import run_dashboard\n",
    "\n",
    "session = TruSession()\n",
    "session.reset_database()\n",
    "# If you have a database you can connect to, use a URL. For example:\n",
    "# session = TruSession(database_url=\"posbtgresql://hostname/database?user=username&password=password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da9a007df154e619e3b16b8fa5498a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard started at http://192.168.0.153:62575 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dashboard(session, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write feedback functions\n",
    "We will now create the feedback functions that will evaluate the app. Remember that the criteria we were evaluating against were:\n",
    "\n",
    "- Ground truth agreement\n",
    "    - For these set of metrics, we will measure how similar the generated summary is to some human-created ground truth.**      \n",
    "- Different measures are used in this example\n",
    "    - BERT score, BLEU, ROUGE and a measure where an LLM is prompted to produce a similarity score.   \n",
    "        - BLEU is not relevant to our application. \n",
    "    - Groundedness\n",
    "        - For this measure, we will estimate if the generated summary can be traced back to parts of the original transcript. ( Not necessary )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Golden Dataset ( Ground Truth Evaluation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_set = (\n",
    "    dev_df[[\"dialogue\", \"summary\"]] \n",
    "    .rename(columns={\"dialogue\": \"query\", \"summary\": \"response\"})\n",
    "    .to_dict(\"records\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': \"#Person1#: Hello, how are you doing today?\\n#Person2#: I ' Ve been having trouble breathing lately.\\n#Person1#: Have you had any type of cold lately?\\n#Person2#: No, I haven ' t had a cold. I just have a heavy feeling in my chest when I try to breathe.\\n#Person1#: Do you have any allergies that you know of?\\n#Person2#: No, I don ' t have any allergies that I know of.\\n#Person1#: Does this happen all the time or mostly when you are active?\\n#Person2#: It happens a lot when I work out.\\n#Person1#: I am going to send you to a pulmonary specialist who can run tests on you for asthma.\\n#Person2#: Thank you for your help, doctor.\",\n",
       "  'response': '#Person2# has trouble breathing. The doctor asks #Person2# about it and will send #Person2# to a pulmonary specialist.'},\n",
       " {'query': \"#Person1#: Hey Jimmy. Let's go workout later today.\\n#Person2#: Sure. What time do you want to go?\\n#Person1#: How about at 3:30?\\n#Person2#: That sounds good. Today we work on Legs and forearm.\\n#Person1#: Hey. I just played basketball earlier, so my legs are a little sore. Let's work out on arms and stomach today.\\n#Person2#: I'm on a weekly schedule. You're messing everything up.\\n#Person1#: C'mon. We're only switching two days. You can do legs on Friday.\\n#Person2#: Aright. I'll meet you at the gym at 3:30 then.\",\n",
       "  'response': '#Person1# invites Jimmy to go workout and persuades him into working out on arms and stomach.'},\n",
       " {'query': \"#Person1#: I need to stop eating such unhealthy foods.\\n#Person2#: I know what you mean. I've started eating better myself.\\n#Person1#: What foods do you eat now?\\n#Person2#: I tend to stick to fruits, vegetables, and chicken.\\n#Person1#: Those are the only things you eat?\\n#Person2#: That's basically what I eat.\\n#Person1#: Why aren't you eating anything else?\\n#Person2#: Well, fruits and vegetables are very healthy.\\n#Person1#: And the chicken?\\n#Person2#: It's really healthy to eat when you bake it.\\n#Person1#: I guess that does sound a lot healthier.\",\n",
       "  'response': \"#Person1# plans to stop eating unhealthy foods, and #Person2# shares #Person2#'s healthy recipe with #Person1#.\"},\n",
       " {'query': \"#Person1#: Do you believe in UFOs?\\n#Person2#: Of course, they are out there.\\n#Person1#: But I never saw them.\\n#Person2#: Are you stupid? They are called UFOs, so not everybody can see them.\\n#Person1#: You mean that you can them.\\n#Person2#: That's right. I can see them in my dreams.\\n#Person1#: They come to the earth?\\n#Person2#: No. Their task is to send the aliens here from the outer space.\\n#Person1#: Aliens from the outer space? Do you talk to them? What do they look like?\\n#Person2#: OK, OK, one by one, please! They look like robots, but they can speak. Their mission is to make friends with human beings.\\n#Person1#: That means that you talk to them? In which language?\\n#Person2#: Of course in English, they learn English on Mars too.\\n#Person1#: Wow. Sounds fantastic!\",\n",
       "  'response': \"#Person2# believes in UFOs and can see them in dreams. #Person1# asks #Person2# about UFOs and aliens in #Person2#'s dreams and finds #Person2#'s dreams fantastic.\"},\n",
       " {'query': \"#Person1#: Did you go to school today?\\n#Person2#: Of course. Did you?\\n#Person1#: I didn't want to, so I didn't.\\n#Person2#: That's sad, but have you gone to the movies recently?\\n#Person1#: That's a switch.\\n#Person2#: I'm serious, have you?\\n#Person1#: No, I haven't. Why?\\n#Person2#: I really want to go to the movies this weekend.\\n#Person1#: So go then.\\n#Person2#: I really don't want to go by myself.\\n#Person1#: Well anyway, do you plan on going to school tomorrow?\\n#Person2#: No, I think I'm going to go to the movies.\",\n",
       "  'response': \"#Person1# didn't go to school today. #Person2# wants to skip class tomorrow to go to the movies.\"},\n",
       " {'query': \"#Person1#: Honey, I think you should quit smoking.\\n#Person2#: Why? You said I was hot when smoking.\\n#Person1#: But I want you to be fit.\\n#Person2#: Smoking is killing. I know.\\n#Person1#: Check out this article. It says smoking can lead to lung cancer.\\n#Person2#: I don't believe it.\\n#Person1#: But you know that smoking does harm to health, right?\\n#Person2#: Of course I know it, but you know it's hard to quit smoking. . .\\n#Person1#: Stop beating around the bush. Will you quit or not?\\n#Person2#: Yes, ma'am. Whatever you say.\",\n",
       "  'response': \"#Person1# asks #Person2# to quit smoking for health. #Person2# thinks it's hard but agrees.\"},\n",
       " {'query': \"#Person1#: Excuse me, Mr. White? I just need you to sign these before I leave.\\n#Person2#: Sure, Sherry. Sorry to have kept you waiting. If you hadn't told me, I probably would have just forgotten all about them.\\n#Person1#: That's my job, sir. Just one more signature here, please.\\n#Person2#: There you are.\",\n",
       "  'response': 'Sherry reminds Mr. White to sign.'},\n",
       " {'query': \"#Person1#: Hey, Karen. Look like you got some sun this weekend.\\n#Person2#: Yeah? I guess so. I spent the weekend at beach.\\n#Person1#: That's great. Where did you stay?\\n#Person2#: Some friends of my parents live out there, and they invited me there.\\n#Person1#: So, what did you do out there? I mean besides bask in the sun, obviously.\\n#Person2#: I jogged up and down the beach and played volleyball. You know I never realized how hard it is to run on sand. I couldn't get through a whole game before I had to sit down.\\n#Person1#: Not to mention cooler. Did you go swimming?\\n#Person2#: I wanted to, but the water is too cold, and I just wetted in up to my knees.\\n#Person1#: It all sounds so relaxing. I wish I could get away to the beach like that.\\n#Person2#: It looks like you could use it. Don't tell me you spent the weekend in the library again.\",\n",
       "  'response': \"#Person1# asks Karen where Karen stayed and how Karen spent the weekend at the beach. #Person1# thinks it's relaxing and wishes to go there.\"},\n",
       " {'query': \"#Person1#: How do you usually spend your leisure time? I mean, do you have any special interests out of your job?\\n#Person2#: Of course. You see, almost everyone has some kind of hobby\\n#Person1#: Yeah, you're quite right and what's your hobby?\\n#Person2#: I like taking photos out of door.\\n#Person1#: Oh, photography, It's really a good hobby.\\n#Person2#: Yes, I usually develop and print all my own photos.\\n#Person1#: You yourself have a photo studio?\\n#Person2#: Yes, simple as it is. It does work.\",\n",
       "  'response': \"#Person1# asks about #Person2#'s hobbies. #Person2# likes photography and has a photo studio.\"},\n",
       " {'query': \"#Person1#: have you ever seen Bill Gate's home on the internet?\\n#Person2#: no. what's it like?\\n#Person1#: it's got its own library, theatre, swimming pool, and a guest house. The house itself has about ten different rooms that are all hooked up to computers so you can get things done in each room through.\\n#Person2#: would you want to live there?\\n#Person1#: I think his house is fantastic, but I wouldn't want to live there. You would have to hire one or two people to clean all the rooms in the house, plus a few people to take care of the gardens.\\n#Person2#: what's your dream home like then?\\n#Person1#: my dream home is actually just a small cottage in a quite village in England.\\n#Person2#: would you want to buy an old cottage or build a new one yourself?\\n#Person1#: old homes are great because they've got character. I think that's important.\\n#Person2#: it that why you wear second-hand cloths as well? Because they've got character?\\n#Person1#: no, that's just because I don't have enough money to buy new cloths all the time!\\n#Person2#: I see. If you lived in an old house, would it be decorated in a modern way?\\n#Person1#: no, I'd definitely try to restore it to its original state. I love to imagine what it'd be like to live in another time in history and living in a house decorated like it would have been 200 years a\",\n",
       "  'response': \"#Person1# and #Person2# talk about Bill Gate's home. #Person1# thinks it's fantastic but wouldn't want to live there. #Person2# asks about #Person1#'s dream house. #Person1# wants to live in a small and old cottage for its character.\"}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_set[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the feedback functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trulens.providers.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ In Similarity (LLM), input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Similarity (LLM), input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In bert_score, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In bert_score, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In bleu, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In bleu, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In rouge, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In rouge, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Groundedness - LLM Judge, input source will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Groundedness - LLM Judge, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Groundedness - NLI Judge, input source will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Groundedness - NLI Judge, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
      "✅ In Comprehensiveness, input source will be set to __record__.main_input or `Select.RecordInput` .\n",
      "✅ In Comprehensiveness, input summary will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " {'reasons': 'Score: 3\\nKey Point: Obama is the president.\\nSupporting Evidence: The summary explicitly states \"obama is the president,\" which fully captures the key point.\\n\\n'})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trulens.core import Select, Feedback\n",
    "from trulens.providers.huggingface import Huggingface\n",
    "from trulens.providers.openai import OpenAI\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "\n",
    "provider = OpenAI(model_engine=\"gpt-4o\")\n",
    "hug_provider = Huggingface() # for groundedness_measure_with_nli \n",
    "\n",
    "ground_truth_collection = GroundTruthAgreement(golden_set, provider=provider)\n",
    "f_groundtruth = Feedback(\n",
    "    ground_truth_collection.agreement_measure, name=\"Similarity (LLM)\"\n",
    ").on_input_output()\n",
    "\n",
    "f_bert_score = Feedback(ground_truth_collection.bert_score).on_input_output()\n",
    "f_bleu = Feedback(ground_truth_collection.bleu).on_input_output()\n",
    "f_rouge = Feedback(ground_truth_collection.rouge).on_input_output()\n",
    "# Groundedness between each context chunk and the response.\n",
    "\n",
    "\n",
    "f_groundedness_llm = (\n",
    "    Feedback(\n",
    "        provider.groundedness_measure_with_cot_reasons,\n",
    "        name=\"Groundedness - LLM Judge\",\n",
    "    )\n",
    "    .on(Select.RecordInput)\n",
    "    .on(Select.RecordOutput)\n",
    ")\n",
    "\n",
    "f_groundedness_nli = (\n",
    "    Feedback(\n",
    "        hug_provider.groundedness_measure_with_nli,\n",
    "        name=\"Groundedness - NLI Judge\",\n",
    "    )\n",
    "    .on(Select.RecordInput)\n",
    "    .on(Select.RecordOutput)\n",
    ")\n",
    "\n",
    "f_comprehensiveness = (\n",
    "    Feedback(\n",
    "        provider.comprehensiveness_with_cot_reasons, name=\"Comprehensiveness\"\n",
    "    )\n",
    "    .on(Select.RecordInput)\n",
    "    .on(Select.RecordOutput)\n",
    ")\n",
    "provider.comprehensiveness_with_cot_reasons(\n",
    "    \"the white house is white. obama is the president\",\n",
    "    \"the white house is white. obama is the president\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up the application with feedback functions\n",
    "Now we are ready to wrap our summarization app with TruLens as a TruCustomApp.   \n",
    "Now each time it will be called, TruLens will log inputs, outputs and any instrumented intermediate steps and evaluate them ith the feedback functions we created.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A customer contacted Amazon customer service regarding a missing page in \"The Paper Bag Night of the Hunter\" by R.A. Salvatore, and was instructed to upload a photo of the issue to receive a replacement without returning the original book.\n"
     ]
    }
   ],
   "source": [
    "summary_app = DialogSummaryApp()\n",
    "print(summary_app.summarize(dev_df.dialogue[498])) # 카테고리하는걸로 변경\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_recorder = TruCustomApp(\n",
    "    summary_app,\n",
    "    app_name=\"Summarization example\",\n",
    "    app_version=\"v1\",\n",
    "    feedbacks=[\n",
    "        f_groundtruth,\n",
    "        f_groundedness_llm,\n",
    "        # f_groundedness_nli,\n",
    "        f_comprehensiveness,\n",
    "        f_bert_score,\n",
    "        f_bleu,\n",
    "        f_rouge,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the summaries for the first 50 records "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaeyeopchung/.pyenv/versions/test-en/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/jaeyeopchung/.pyenv/versions/test-en/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/jaeyeopchung/.pyenv/versions/test-en/lib/python3.10/site-packages/trulens/feedback/llm_provider.py:1521: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaeyeopchung/.pyenv/versions/test-en/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/jaeyeopchung/.pyenv/versions/test-en/lib/python3.10/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#  test a single run of the App as so. This should show up on the dashboard.\n",
    "for row in golden_set[:50]:\n",
    "    with summary_recorder:\n",
    "        summary_app.summarize(dialog=row['query'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optionally, tenacity can be used to retry the app if it fails to run\n",
    "We'll make a lot of queries in a short amount of time, so we need tenacity to make sure that most of our requests eventually go through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tenacity import retry\n",
    "# from tenacity import stop_after_attempt\n",
    "# from tenacity import wait_random_exponential\n",
    "\n",
    "# @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "# def run_with_backoff(doc):\n",
    "#     return summary_recorder.with_record(summary_app.summarize, dialog=doc)\n",
    "\n",
    "# for pair in golden_set[:100]:\n",
    "#     llm_response = run_with_backoff(pair[\"query\"])\n",
    "#     print(llm_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Comprehensiveness</th>\n",
       "      <th>Groundedness - LLM Judge</th>\n",
       "      <th>Groundedness - NLI Judge</th>\n",
       "      <th>latency</th>\n",
       "      <th>total_cost</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>app_name</th>\n",
       "      <th>app_version</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Summarization example</th>\n",
       "      <th>v1</th>\n",
       "      <td>0.679861</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.940408</td>\n",
       "      <td>1.661563</td>\n",
       "      <td>0.001037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Comprehensiveness  \\\n",
       "app_name              app_version                      \n",
       "Summarization example v1                    0.679861   \n",
       "\n",
       "                                   Groundedness - LLM Judge  \\\n",
       "app_name              app_version                             \n",
       "Summarization example v1                                1.0   \n",
       "\n",
       "                                   Groundedness - NLI Judge   latency  \\\n",
       "app_name              app_version                                       \n",
       "Summarization example v1                           0.940408  1.661563   \n",
       "\n",
       "                                   total_cost  \n",
       "app_name              app_version              \n",
       "Summarization example v1             0.001037  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.get_leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n",
      "Dashboard already running at path:   Network URL: http://192.168.0.153:62575\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Popen: returncode: None args: ['streamlit', 'run', '--server.headless=True'...>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_dashboard(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
