{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Crawl4ai\n",
    "- The product pages on the AXA website contain /pk/ in their URLs (e.g., https://www.axa.de/pk/gesundheit/p/zahnzusatzversicherung).\n",
    "- This specific pattern was used to filter internal links from the homepage.\n",
    "### Next:\n",
    "- Integrate with Agents\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "import os\n",
    "\n",
    "# Semaphore to limit concurrency\n",
    "SEMAPHORE = asyncio.Semaphore(2)  # Adjust to 3 for better performance\n",
    "\n",
    "# Function to fetch product page links from the homepage\n",
    "async def fetch_product_links(limit=200):\n",
    "    async with AsyncWebCrawler() as crawler:\n",
    "        result = await crawler.arun(url=\"https://www.axa.de\")\n",
    "        internal_links = result.links['internal']\n",
    "        # Filter links that point to product pages containing '/pk/'\n",
    "        product_links = [link['href'] for link in internal_links if '/pk/' in link['href']]\n",
    "        return product_links[:limit]\n",
    "\n",
    "# Function to crawl a page and save it as Markdown\n",
    "async def crawl_and_save(link, session, output_dir=\"markdownPages\"):\n",
    "    async with SEMAPHORE:  # Limit concurrent tasks\n",
    "        async with AsyncWebCrawler(session=session) as crawler:\n",
    "            result = await crawler.arun(url=link)\n",
    "            if result.success:\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                filename = os.path.join(output_dir, f\"{link.split('/')[-1] or 'index'}.md\")\n",
    "                with open(filename, 'w', encoding='utf-8') as file:\n",
    "                    file.write(result.markdown)\n",
    "                print(f\"Saved: {filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to crawl: {link}\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    print(\"Fetching product page links...\")\n",
    "    links = await fetch_product_links(limit=200)\n",
    "    print(f\"Found {len(links)} product page links.\")\n",
    "\n",
    "    async with AsyncWebCrawler() as session:\n",
    "        tasks = [crawl_and_save(link, session) for link in links]\n",
    "        await asyncio.gather(*tasks)\n",
    "    print(\"Crawling and saving completed.\")\n",
    "\n",
    "\n",
    "await main()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
