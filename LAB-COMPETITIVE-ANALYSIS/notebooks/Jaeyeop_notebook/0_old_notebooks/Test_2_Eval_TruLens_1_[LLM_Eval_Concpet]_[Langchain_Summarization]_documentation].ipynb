{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating the accuracy of the model \n",
    "## Evaluation Concepts & Metrics  \n",
    "[Evaluation Concept - Langchain Documnet](https://docs.smith.langchain.com/evaluation/concepts)  \n",
    "[LLM Evaluation Metrics](https://arize.com/blog-course/llm-evaluation-the-definitive-guide/)  \n",
    "[TruLens Evaluation Blog](https://gowrishankar.info/blog/evaluating-large-language-models-generated-contents-with-trueras-trulens/)\n",
    "\n",
    "My guess was the best metric to use is the answer relevancy for our use-case, but relevance metric requires retrieval(RAG) to evaluate the prompts, which is not suitable for this task.  ( Johann confirmed this on 29. Nov 2024 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other reference regarding summarization \n",
    "Summarization tools Langchain offers : \n",
    "- Stuff / Map-reduce&refine / CoD / Clustering Map-Refine \n",
    "- [Stuff / Map-Reduce document](https://python.langchain.com/docs/modules/data_connection/document_transformers/map_reduce) \n",
    "\n",
    "Summarization tools offered by LangChain : \n",
    "- Stuff / Map-reduce&refine / CoD / Clustering Map-Refine    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trulens Evaluation \n",
    "- [LangChain Quickstart](https://www.trulens.org/getting_started/quickstarts/langchain_quickstart/#create-rag) \n",
    "- [Text to Text](https://www.trulens.org/getting_started/quickstarts/text2text_quickstart/) \n",
    "    - Can be used for our task, but how to use it for evaluation? \n",
    "- [Ground Truth Evaluation](https://www.trulens.org/getting_started/quickstarts/groundtruth_evals/) \n",
    "    - Can be used for our task, how to generate the ground truth data set for our task? ( Tried with the RAG based evalation but not suitable for our case )\n",
    "- [Feedback Function Implementations](https://www.trulens.org/component_guides/evaluation/feedback_implementations/)  \n",
    "    - [Classification-based](https://www.trulens.org/component_guides/evaluation/feedback_implementations/stock/)  \n",
    "    - [context_relevance_with_cot_reasons](https://www.trulens.org/reference/trulens/providers/openai/#trulens.providers.openai.OpenAI.context_relevance_with_cot_reasons)  \n",
    "    - [context_relevance](https://www.trulens.org/reference/trulens/providers/langchain/provider/#trulens.providers.langchain.provider.Langchain.context_relevance)   \n",
    "    - Some feedback functions require retrieval(RAG) to evaluate the prompts (BaseRetriever), which is not suitable for our task !!  \n",
    "\n",
    "\n",
    "[TruLens Video 1](https://www.youtube.com/watch?v=ul5huLywzZk )  \n",
    "[TruLens Vidoe 2](https://www.youtube.com/watch?v=8NP1HLrNuAo)  \n",
    "\n",
    "\n",
    "### [TODO] Keep in mind that our application is not a RAG based application and find the right metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " #### [Trulens Example ](https://www.trulens.org/getting_started/quickstarts/langchain_quickstart/#create-rag)\n",
    "    - RAG based evaluation, which is not suitable for this task. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building RAG \n",
    "Doing it without RAG seems to be possible but I failed to find the right resources / documentation.   \n",
    "Comparing only the relvance of input and output does not make sense for this task. ( e.g., is this a insurance product? / yes or no => no relevance ) \n",
    "- Test_Evaluation notebook \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('insurance_company_db.csv') # load the categorized data \n",
    "\n",
    "# Convert DataFrame to documents\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=f\"{row['content']}\", \n",
    "        metadata={\"source\": row['source'], \"company\": row['company']}\n",
    "    ) for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize embeddings with caching\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "store = LocalFileStore('./cache/')\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings=embeddings_model,\n",
    "    document_embedding_cache=store,\n",
    "    namespace=embeddings_model.model\n",
    ")\n",
    "\n",
    "# Create and save FAISS vector store\n",
    "vectorstore = FAISS.from_documents(split_documents, cached_embedder)\n",
    "vectorstore.save_local('./db/insurance_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Based on this query, one can build a test set \n",
    "- Building a test data set ( Ground Truth Data set )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7_/lzvh2hfd7nbfj2n6q9k0hb980000gn/T/ipykernel_72469/657633760.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm=ChatOpenAI(temperature=0),\n",
      "/var/folders/7_/lzvh2hfd7nbfj2n6q9k0hb980000gn/T/ipykernel_72469/657633760.py:21: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(\"Answer:\", qa_chain.run(query))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Doex AXA Germany offer health insurance?\n",
      "Answer: Yes, AXA Germany offers health insurance.\n",
      "\n",
      "Query: Genrali offers Auto Insurance?\n",
      "Answer: Yes, Generali offers Auto Insurance as one of their products.\n",
      "\n",
      "Query: HUK-COBURG offers Health insurance?\n",
      "Answer: Based on the provided context, HUK-COBURG offers accident insurance, but there is no specific mention of health insurance. Therefore, it is unclear if HUK-COBURG offers health insurance.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved vectorstore\n",
    "vectorstore = FAISS.load_local('./db/insurance_data', cached_embedder, allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Example queries\n",
    "queries = [\n",
    "    \"Doex AXA Germany offer health insurance?\",\n",
    "    \"Genrali offers Auto Insurance?\",\n",
    "    \"HUK-COBURG offers Health insurance?\"\n",
    "]\n",
    "\n",
    "# Run queries\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"Answer:\", qa_chain.run(query))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Idea to evaluate the prompt ( [Ground Truth Evaluation](https://www.trulens.org/getting_started/quickstarts/groundtruth_evals/) )\n",
    "-> What should be the ground truth for this task?   \n",
    "\n",
    "My idea : \n",
    "1. Build a RAG based on the categorized data \n",
    "2. Build test data set ( QA based test set ) from the RAG  --> Use this as the ground truth ?? \n",
    "\n",
    "3. Use different prompts to categorize the companies \n",
    "4. Use the test data set ( from 2 ) to evaluate how good the prompt is. For example, if A was a insurance company from 1~2, the the categorized data set should also have A as a insurance company.  \n",
    "\n",
    "--> But 2 cannot be a ground truth since the test set is built from another prompt.\n",
    "--> How should we evaluate the quality of the prompt then?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦‘ Initialized with db url sqlite:///default.sqlite .\n",
      "ðŸ›‘ Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from trulens.apps.custom import instrument\n",
    "from trulens.core import TruSession\n",
    "\n",
    "session = TruSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01manswer_format\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[1;32m      2\u001b[0m     answer : \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer to the question\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, api_key\u001b[38;5;241m=\u001b[39mOpenAI_key, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16384\u001b[39m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "class answer_format(BaseModel):\n",
    "    answer : str = Field(description=\"answer to the question\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=OpenAI_key, temperature=0, max_tokens=16384) \n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=answer_format)\n",
    "format_instructions = parser.get_format_instructions()\n",
    "\n",
    "\n",
    "class APP:\n",
    "    @instrument\n",
    "    def completion(self, prompt):\n",
    "        system_input_stage_1 = read_prompt(prompt_path + \"classification_system_stage_1.txt\")\n",
    "        df = pd.read_csv('company_data.csv')\n",
    "\n",
    "        completion = (\n",
    "            OpenAI().chat.completions.create(\n",
    "                model = \"gpt-4o\",\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_input_stage_1},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format = answer_format,\n",
    "                format_instructions = format_instructions\n",
    "            ).choices[0].message.content\n",
    "        )\n",
    "        return completion\n",
    "    \n",
    "llm_app = APP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… In Ground Truth Semantic Agreement, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
      "âœ… In Ground Truth Semantic Agreement, input response will be set to __record__.main_output or `Select.RecordOutput` .\n"
     ]
    }
   ],
   "source": [
    "# Initialize Feedback Function \n",
    "from trulens.core import Feedback\n",
    "from trulens.feedback import GroundTruthAgreement\n",
    "from trulens.providers.openai import OpenAI as fOpenAI\n",
    "\n",
    "golden_set = [\n",
    "    {\n",
    "        \"query\": \"Doex AXA Germany offer health insurance?\",\n",
    "        \"expected_response\": \"Yes, AXA Germany offers health insurance.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Genrali offers Auto Insurance?\",\n",
    "        \"expected_response\": \"Yes, Generali offers Auto Insurance as one of their products.\",\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"HUK-COBURG offers Health insurance?\",\n",
    "        \"expected_response\": \" Based on the provided context, HUK-COBURG offers accident insurance, liability insurance, and support for driving officials. There is no specific mention of health insurance in the information provided.\",\n",
    "    }\n",
    "]\n",
    "\n",
    "f_groundtruth = Feedback(\n",
    "    GroundTruthAgreement(golden_set, provider=fOpenAI()).agreement_measure,\n",
    "    name=\"Ground Truth Semantic Agreement\",\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm_app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# add trulens as a context manager for llm_app\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrulens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapps\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcustom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TruCustomApp\n\u001b[1;32m      4\u001b[0m tru_app \u001b[38;5;241m=\u001b[39m TruCustomApp(\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mllm_app\u001b[49m, app_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLM App\u001b[39m\u001b[38;5;124m\"\u001b[39m, app_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv1\u001b[39m\u001b[38;5;124m\"\u001b[39m, feedbacks\u001b[38;5;241m=\u001b[39m[f_groundtruth]\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'llm_app' is not defined"
     ]
    }
   ],
   "source": [
    "# add trulens as a context manager for llm_app\n",
    "from trulens.apps.custom import TruCustomApp\n",
    "\n",
    "tru_app = TruCustomApp(\n",
    "    llm_app, app_name=\"LLM App\", app_version=\"v1\", feedbacks=[f_groundtruth]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tru_app' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Instrumented query engine can operate as a context manager:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtru_app\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m recording:\n\u001b[1;32m      3\u001b[0m     llm_app\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoes AXA Germany offer health insurance?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     llm_app\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDoes HUK-COBURG offer health insurance?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tru_app' is not defined"
     ]
    }
   ],
   "source": [
    "# Instrumented query engine can operate as a context manager:\n",
    "with tru_app as recording:\n",
    "    llm_app.completion(\"Does AXA Germany offer health insurance?\")\n",
    "    llm_app.completion(\"Does HUK-COBURG offer health insurance?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-en",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
